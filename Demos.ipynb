{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from api import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "\n",
    "1. Enter keys in config-demo.py and rename the file to config.py\n",
    "2. All keys are not needed for all API services.\n",
    "3. You can set PROXY_ENPOINT to '' to bypass using a proxy.  Otherwise, could be used for rendertron (eg 'https://my-project.appspot.com/render/<URL\\>' )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GSC Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading Existing: data/demo2.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clicks</th>\n",
       "      <th>clientID</th>\n",
       "      <th>ctr</th>\n",
       "      <th>impressions</th>\n",
       "      <th>month</th>\n",
       "      <th>page</th>\n",
       "      <th>position</th>\n",
       "      <th>query</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.0</td>\n",
       "      <td>https://adaptpartners.com</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2018-10</td>\n",
       "      <td>https://adaptpartners.com/</td>\n",
       "      <td>1</td>\n",
       "      <td>adapt partners</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>https://adaptpartners.com</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2018-10</td>\n",
       "      <td>https://adaptpartners.com/job/political-journa...</td>\n",
       "      <td>2</td>\n",
       "      <td>political internships</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>https://adaptpartners.com</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>19.0</td>\n",
       "      <td>2018-10</td>\n",
       "      <td>https://adaptpartners.com/technical-seo/python...</td>\n",
       "      <td>4</td>\n",
       "      <td>google search console api</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>https://adaptpartners.com</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2018-10</td>\n",
       "      <td>https://adaptpartners.com/</td>\n",
       "      <td>13</td>\n",
       "      <td>adapt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>https://adaptpartners.com</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2018-10</td>\n",
       "      <td>https://adaptpartners.com/</td>\n",
       "      <td>14</td>\n",
       "      <td>adapt agency</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   clicks                   clientID       ctr  impressions    month  \\\n",
       "0     2.0  https://adaptpartners.com  0.666667          3.0  2018-10   \n",
       "1     2.0  https://adaptpartners.com  0.400000          5.0  2018-10   \n",
       "2     1.0  https://adaptpartners.com  0.052632         19.0  2018-10   \n",
       "3     0.0  https://adaptpartners.com  0.000000         11.0  2018-10   \n",
       "4     0.0  https://adaptpartners.com  0.000000          1.0  2018-10   \n",
       "\n",
       "                                                page  position  \\\n",
       "0                         https://adaptpartners.com/         1   \n",
       "1  https://adaptpartners.com/job/political-journa...         2   \n",
       "2  https://adaptpartners.com/technical-seo/python...         4   \n",
       "3                         https://adaptpartners.com/        13   \n",
       "4                         https://adaptpartners.com/        14   \n",
       "\n",
       "                       query  \n",
       "0             adapt partners  \n",
       "1      political internships  \n",
       "2  google search console api  \n",
       "3                      adapt  \n",
       "4               adapt agency  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsc_property = 'https://adaptpartners.com'\n",
    "days_back = 30\n",
    "\n",
    "'''\n",
    "Parameters:\n",
    "\n",
    "Positional:\n",
    "clienturl: (str) The domain URL property name in Google Search Console.\n",
    "days_back: (int) How many days history to pull.\n",
    "\n",
    "Keyword:\n",
    "thresholdtype: (str)  'click' or 'impression'. Default: impression\n",
    "threshold: (int) Keep pulling, daily until less than this number of either clicks or impressions. Default: 1\n",
    "poslimit: (int) Omit results above this limit. Default: None\n",
    "country: (str) Country. Default: usa\n",
    "outputFn: (str) Name of the output file.  If not set, a unique name will be chosen.\n",
    "'''\n",
    "\n",
    "df = gscservice.get_site_data(gsc_property, days_back, output_fn=\"demo2.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## URL Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading url https://adaptpartners.com/technical-seo/python-notebooks-connect-to-google-search-console-api-and-extract-data/\n",
      "Extracted 509 words.\n",
      "\n",
      "\n",
      "Title: Python Notebooks: Connect to Google Search Console API and Extract Data\n",
      "H1: ['Python Notebooks: Connect to Google Search Console API and Extract Data']\n",
      "Extracted Text: On this post we want to show you an easy way that you can use Python notebooks to connect to Google’s Search Console API. After connecting to the API, you will be able to do several interesting things.\n",
      "\n",
      "The first thing you need is to create a new Oauth Credential in Google Developers Console and select “Other” as type. Google provides detailed information on how to set this up here.\n",
      "\n",
      "After completing these steps you’ll have a CLIENT_ID and CLIENT_SECRET that you will need to use in this notebook in order to connect to Google Search Console. Dominic Woodman’s post in Moz’s blog shows easy step by step instructions on how you can set this up on Google.\n",
      "\n",
      "Google provides information on how to use Python to connect to their API, however the code they provide on this page is in Python 2. We went ahead and updated this code to Python 3 and added a few changes so that the credentials are saved so that you don’t need to plug in the verification code each time you run the code.\n",
      "\n",
      "With the code above you’ll be able to connect with Google Search Console. Now, you’ll be able to do several interesting things by connecting to this API:\n",
      "• Search Analytics: You’ll be able to extract information from the search analytics report. With the “Query” method you can obtain your search traffic data. You can add filters, date range and parameters in order to extract the data you need. The method returns zero or more rows grouped by the row keys (dimensions) that you define. For more detailed information and examples, please see https://developers.google.com/webmaster-tools/search-console-api-original/v3/searchanalytics#resource\n",
      "• Sitemaps: You are able to delete sitemap from sites that you select, retrieves information about a specific sitemap with the get method, list the sitemaps-entries submitted for a site, or included in the sitemap index file with the list method. You can also submit a new sitemap for a site by using the submit method. For detailed information see: https://developers.google.com/webmaster-tools/search-console-api-original/v3/sitemaps#resource\n",
      "• URL Crawl Error Counts: From the crawl error report, you are able to retrieve a time series of the number of URL crawl errors per error category (404, soft 404s, 50x errors, etc) and platform (web, smartphone, etc.) with the query method. https://developers.google.com/webmaster-tools/search-console-api-original/v3/urlcrawlerrorscounts#resource\n",
      "• URL Crawl Errors Samples: From the same crawl error report, you can retrieve details about crawl errors for a site’s sample URL. Here is a great example of that JR Oakes did where he uses the API to find through the crawl errors pages that are linked externally and should be redirected.. You can also extract a list a site’s sample URLs for the specified crawl error category and platform. It is also possible to mark crawl errors as fixed and removes it from the sample list. https://developers.google.com/webmaster-tools/search-console-api-original/v3/urlcrawlerrorssamples#resource\n",
      "\n",
      "As you can see there are several things that can be done when connecting to Google’s search console.\n",
      "\n",
      "You might also be interested in reading JR’s post about saving Google Search Console data to BigQuery.\n",
      "\n",
      "If you have other examples you would like to share, please link to them on the comments.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "url = 'https://adaptpartners.com/technical-seo/python-notebooks-connect-to-google-search-console-api-and-extract-data/'\n",
    "text, infos = extract_url_data(url)\n",
    "print('\\n')\n",
    "print(\"Title:\", infos['title'])\n",
    "print(\"H1:\", infos['h1'])\n",
    "print(\"Extracted Text:\", text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Watson API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading url https://adaptpartners.com/technical-seo/python-notebooks-connect-to-google-search-console-api-and-extract-data/\n",
      "Extracted 509 words.\n",
      "\n",
      " Entities:\n",
      "   count                                     disambiguation  relevance  \\\n",
      "0      9  {'subtype': ['AcademicInstitution', 'AwardPres...   0.876669   \n",
      "1      1                                                NaN   0.169787   \n",
      "\n",
      "              text     type  \n",
      "0           Google  Company  \n",
      "1  Dominic Woodman   Person  \n",
      "\n",
      " Keywords:\n",
      "    relevance                       text\n",
      "0    0.984217      Google Search Console\n",
      "1    0.715410  Google Developers Console\n",
      "2    0.653734         Search Console API\n",
      "3    0.573839        Search Console data\n",
      "4    0.562398         interesting things\n",
      "5    0.562259       new Oauth Credential\n",
      "6    0.528074            shows easy step\n",
      "7    0.474809           Python notebooks\n",
      "8    0.447963            Dominic Woodman\n",
      "9    0.443203                   easy way\n",
      "10   0.434548       detailed information\n",
      "11   0.426069          step instructions\n",
      "12   0.420550          verification code\n",
      "13   0.341544                       post\n",
      "14   0.338461                        Moz\n",
      "15   0.325447                       type\n",
      "16   0.325374                      steps\n",
      "17   0.325252                credentials\n",
      "18   0.324879                  CLIENT_ID\n",
      "19   0.324861              CLIENT_SECRET\n",
      "20   0.323923                       page\n",
      "21   0.323567                    changes\n",
      "22   0.323496                   examples\n",
      "23   0.323422                       link\n",
      "24   0.323390                       time\n",
      "25   0.323376                   comments\n"
     ]
    }
   ],
   "source": [
    "url = 'https://adaptpartners.com/technical-seo/python-notebooks-connect-to-google-search-console-api-and-extract-data/'\n",
    "text, infos = extract_url_data(url)\n",
    "html = infos['html']\n",
    "\n",
    "print('\\n Entities:')\n",
    "print(watsonservice.watson_entities(html))\n",
    "\n",
    "print('\\n Keywords:')\n",
    "print(watsonservice.watson_keywords(html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SEMRush"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uses this library: https://github.com/storerjeremy/python-semrush\n",
    "# See Readme for implentation \n",
    "\n",
    "domain = 'adaptpartners.com'\n",
    "database = 'us'\n",
    "ranks = semrushservice.domain_organic(domain, database)\n",
    "ranks.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google Analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>sessions</th>\n",
       "      <th>pageviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-11-07</td>\n",
       "      <td>59</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-11-08</td>\n",
       "      <td>55</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-11-09</td>\n",
       "      <td>38</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-11-10</td>\n",
       "      <td>23</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-11-11</td>\n",
       "      <td>22</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date  sessions  pageviews\n",
       "0  2018-11-07        59         88\n",
       "1  2018-11-08        55         66\n",
       "2  2018-11-09        38         46\n",
       "3  2018-11-10        23         34\n",
       "4  2018-11-11        22         27"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Uses this library: https://github.com/debrouwere/google-analytics/wiki/Querying\n",
    "# See link for usage instructions.\n",
    "\n",
    "ga = gaservice\n",
    "\n",
    "ga_account = \"Adapt Partners\" \n",
    "ga_webproperty = \"Adapt Partners\" \n",
    "ga_profile = \"Adapt Partners\"\n",
    "\n",
    "profile = gaservice.get_profile(account=ga_account, webproperty=ga_webproperty, profile=ga_profile )\n",
    "query = profile.core.query.daily(days=-5)\n",
    "query.metrics('sessions', 'pageviews').as_dataframe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading Existing: data/demo2.csv\n",
      "# training samples: 6115\n",
      "# batches: 191\n"
     ]
    }
   ],
   "source": [
    "# Create dataset from Google Search Console data for usage in Machine Learning projects.\n",
    "\n",
    "import dataset\n",
    "\n",
    "gsc_property = 'https://adaptpartners.com'\n",
    "days_back = 30\n",
    "\n",
    "df = gscservice.get_site_data(gsc_property, days_back, output_fn=\"demo2.csv\")\n",
    "df.head()\n",
    "\n",
    "features = df[['position','impressions']]\n",
    "labels = df[['clicks']]\n",
    "\n",
    "data_loader = dataset.load_pandas(features, labels, batch_size=32, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading Existing: data/demo2.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11/29/2018 08:25:28 - INFO - dataset.bert_ds -   device: cpu n_gpu: 1 distributed training: False\n",
      "11/29/2018 08:25:29 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at C:\\Users\\jroak\\.pytorch_pretrained_bert\\26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "11/29/2018 08:25:29 - INFO - dataset.bert_ds -   *** Example ***\n",
      "11/29/2018 08:25:29 - INFO - dataset.bert_ds -   unique_id: 0\n",
      "11/29/2018 08:25:29 - INFO - dataset.bert_ds -   tokens: [CLS] adapt partners [SEP]\n",
      "11/29/2018 08:25:29 - INFO - dataset.bert_ds -   input_ids: 101 15581 5826 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/29/2018 08:25:29 - INFO - dataset.bert_ds -   input_mask: 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/29/2018 08:25:29 - INFO - dataset.bert_ds -   input_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/29/2018 08:25:29 - INFO - dataset.bert_ds -   *** Example ***\n",
      "11/29/2018 08:25:29 - INFO - dataset.bert_ds -   unique_id: 1\n",
      "11/29/2018 08:25:29 - INFO - dataset.bert_ds -   tokens: [CLS] political internship ##s [SEP]\n",
      "11/29/2018 08:25:29 - INFO - dataset.bert_ds -   input_ids: 101 2576 22676 2015 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/29/2018 08:25:29 - INFO - dataset.bert_ds -   input_mask: 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/29/2018 08:25:29 - INFO - dataset.bert_ds -   input_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/29/2018 08:25:29 - INFO - dataset.bert_ds -   *** Example ***\n",
      "11/29/2018 08:25:29 - INFO - dataset.bert_ds -   unique_id: 2\n",
      "11/29/2018 08:25:29 - INFO - dataset.bert_ds -   tokens: [CLS] google search console api [SEP]\n",
      "11/29/2018 08:25:29 - INFO - dataset.bert_ds -   input_ids: 101 8224 3945 10122 17928 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/29/2018 08:25:29 - INFO - dataset.bert_ds -   input_mask: 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/29/2018 08:25:29 - INFO - dataset.bert_ds -   input_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/29/2018 08:25:29 - INFO - dataset.bert_ds -   *** Example ***\n",
      "11/29/2018 08:25:29 - INFO - dataset.bert_ds -   unique_id: 3\n",
      "11/29/2018 08:25:29 - INFO - dataset.bert_ds -   tokens: [CLS] adapt [SEP]\n",
      "11/29/2018 08:25:29 - INFO - dataset.bert_ds -   input_ids: 101 15581 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/29/2018 08:25:29 - INFO - dataset.bert_ds -   input_mask: 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/29/2018 08:25:29 - INFO - dataset.bert_ds -   input_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/29/2018 08:25:29 - INFO - dataset.bert_ds -   *** Example ***\n",
      "11/29/2018 08:25:29 - INFO - dataset.bert_ds -   unique_id: 4\n",
      "11/29/2018 08:25:29 - INFO - dataset.bert_ds -   tokens: [CLS] adapt agency [SEP]\n",
      "11/29/2018 08:25:29 - INFO - dataset.bert_ds -   input_ids: 101 15581 4034 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/29/2018 08:25:29 - INFO - dataset.bert_ds -   input_mask: 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/29/2018 08:25:29 - INFO - dataset.bert_ds -   input_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/29/2018 08:25:31 - INFO - pytorch_pretrained_bert.modeling -   loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz from cache at C:\\Users\\jroak\\.pytorch_pretrained_bert\\9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n",
      "11/29/2018 08:25:31 - INFO - pytorch_pretrained_bert.modeling -   extracting archive file C:\\Users\\jroak\\.pytorch_pretrained_bert\\9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba to temp dir C:\\Users\\jroak\\AppData\\Local\\Temp\\tmpk0bt_yu6\n",
      "11/29/2018 08:25:37 - INFO - pytorch_pretrained_bert.modeling -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Implements the BERT model for usage in machine learning projects.\n",
    "\n",
    "import dataset\n",
    "\n",
    "gsc_property = 'https://adaptpartners.com'\n",
    "days_back = 30\n",
    "\n",
    "df = gscservice.get_site_data(gsc_property, days_back, output_fn=\"demo2.csv\")\n",
    "df.head()\n",
    "\n",
    "features = df[['position','impressions']]\n",
    "labels = df[['clicks']]\n",
    "\n",
    "def apply_embed(row):\n",
    "    embed = row['embedding']\n",
    "    for i, e in enumerate(embed):\n",
    "        row['e_'+str(i)] = e\n",
    "    return row\n",
    "\n",
    "data_loader_bert, df_bert = dataset.load_bert_df(input_df=df, input_row=\"query\")\n",
    "\n",
    "df_bert_embed = df_bert.apply(apply_embed,axis=1).drop(columns=['embedding','linex_index','tokens'])\n",
    "\n",
    "features = pd.concat([features.reset_index(drop =True), df_bert_embed.reset_index(drop =True)], axis=1)\n",
    "\n",
    "data_loader = dataset.load_pandas(features, labels, batch_size=32, shuffle=True, drop_last=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
